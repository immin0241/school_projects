{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/immin0241/school_projects/blob/master/3_1_ai/ai_jobs_wage_expectation_tuning_1\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf0PlaXZMf3P",
        "outputId": "6bb9012f-80fa-4a99-8c27-515fe0c605de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터 수: 649개, 테스트 데이터 수: 73개\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 86085.1562 - mae: 86085.1562 - val_loss: 91111.4844 - val_mae: 91111.4844\n",
            "Epoch 2/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 83536.3984 - mae: 83536.3984 - val_loss: 80064.5781 - val_mae: 80064.5781\n",
            "Epoch 3/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 63309.8906 - mae: 63309.8906 - val_loss: 34247.0430 - val_mae: 34247.0430\n",
            "Epoch 4/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 28536.8574 - mae: 28536.8574 - val_loss: 29632.7754 - val_mae: 29632.7754\n",
            "Epoch 5/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23858.4980 - mae: 23858.4980 - val_loss: 26342.2383 - val_mae: 26342.2383\n",
            "Epoch 6/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21404.9629 - mae: 21404.9629 - val_loss: 22798.2246 - val_mae: 22798.2246\n",
            "Epoch 7/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 19171.3770 - mae: 19171.3770 - val_loss: 20680.8555 - val_mae: 20680.8555\n",
            "Epoch 8/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 17311.2637 - mae: 17311.2637 - val_loss: 18512.7715 - val_mae: 18512.7715\n",
            "Epoch 9/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17473.3281 - mae: 17473.3281 - val_loss: 16776.7480 - val_mae: 16776.7480\n",
            "Epoch 10/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16095.2109 - mae: 16095.2109 - val_loss: 16135.7754 - val_mae: 16135.7754\n",
            "Epoch 11/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15990.9043 - mae: 15990.9043 - val_loss: 15522.0605 - val_mae: 15522.0605\n",
            "Epoch 12/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16827.5820 - mae: 16827.5820 - val_loss: 15616.0596 - val_mae: 15616.0596\n",
            "Epoch 13/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15955.8682 - mae: 15955.8682 - val_loss: 15057.3232 - val_mae: 15057.3232\n",
            "Epoch 14/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16360.0244 - mae: 16360.0244 - val_loss: 14746.1172 - val_mae: 14746.1172\n",
            "Epoch 15/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 16442.4414 - mae: 16442.4414 - val_loss: 14457.1230 - val_mae: 14457.1230\n",
            "Epoch 16/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15998.5205 - mae: 15998.5205 - val_loss: 14323.3545 - val_mae: 14323.3545\n",
            "Epoch 17/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14948.5420 - mae: 14948.5420 - val_loss: 14206.6357 - val_mae: 14206.6357\n",
            "Epoch 18/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 16793.3203 - mae: 16793.3203 - val_loss: 14357.6650 - val_mae: 14357.6650\n",
            "Epoch 19/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 14876.6816 - mae: 14876.6816 - val_loss: 13880.5059 - val_mae: 13880.5059\n",
            "Epoch 20/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 13626.6992 - mae: 13626.6992 - val_loss: 14365.3711 - val_mae: 14365.3711\n",
            "Epoch 21/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15386.9453 - mae: 15386.9453 - val_loss: 13748.6445 - val_mae: 13748.6445\n",
            "Epoch 22/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 14957.5938 - mae: 14957.5938 - val_loss: 13374.4639 - val_mae: 13374.4639\n",
            "Epoch 23/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14913.0742 - mae: 14913.0742 - val_loss: 13576.8447 - val_mae: 13576.8447\n",
            "Epoch 24/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 14929.1670 - mae: 14929.1670 - val_loss: 13899.9580 - val_mae: 13899.9580\n",
            "Epoch 25/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 14942.3789 - mae: 14942.3789 - val_loss: 13514.2363 - val_mae: 13514.2363\n",
            "Epoch 26/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14451.1211 - mae: 14451.1211 - val_loss: 13501.5244 - val_mae: 13501.5244\n",
            "Epoch 27/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16181.7246 - mae: 16181.7246 - val_loss: 13464.1914 - val_mae: 13464.1914\n",
            "Epoch 28/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14588.3154 - mae: 14588.3154 - val_loss: 13507.4902 - val_mae: 13507.4902\n",
            "Epoch 29/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14123.0195 - mae: 14123.0195 - val_loss: 13264.1152 - val_mae: 13264.1152\n",
            "Epoch 30/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14656.1553 - mae: 14656.1553 - val_loss: 13297.2627 - val_mae: 13297.2627\n",
            "Epoch 31/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15285.2021 - mae: 15285.2021 - val_loss: 13160.6113 - val_mae: 13160.6113\n",
            "Epoch 32/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13629.2744 - mae: 13629.2744 - val_loss: 13469.5615 - val_mae: 13469.5615\n",
            "Epoch 33/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14800.7520 - mae: 14800.7520 - val_loss: 13253.9658 - val_mae: 13253.9658\n",
            "Epoch 34/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14610.6553 - mae: 14610.6553 - val_loss: 13070.1836 - val_mae: 13070.1836\n",
            "Epoch 35/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14900.5352 - mae: 14900.5352 - val_loss: 12991.1904 - val_mae: 12991.1904\n",
            "Epoch 36/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 14739.9727 - mae: 14739.9727 - val_loss: 13262.2744 - val_mae: 13262.2744\n",
            "Epoch 37/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14961.6592 - mae: 14961.6592 - val_loss: 13003.0996 - val_mae: 13003.0996\n",
            "Epoch 38/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14403.9385 - mae: 14403.9385 - val_loss: 12887.3730 - val_mae: 12887.3730\n",
            "Epoch 39/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14529.3350 - mae: 14529.3350 - val_loss: 12941.9961 - val_mae: 12941.9961\n",
            "Epoch 40/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13478.5771 - mae: 13478.5771 - val_loss: 12809.1006 - val_mae: 12809.1006\n",
            "Epoch 41/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14819.9297 - mae: 14819.9297 - val_loss: 12890.1924 - val_mae: 12890.1924\n",
            "Epoch 42/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14605.2090 - mae: 14605.2090 - val_loss: 12999.0273 - val_mae: 12999.0273\n",
            "Epoch 43/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13706.7412 - mae: 13706.7412 - val_loss: 12957.1396 - val_mae: 12957.1396\n",
            "Epoch 44/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14272.9648 - mae: 14272.9648 - val_loss: 12968.8340 - val_mae: 12968.8340\n",
            "Epoch 45/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14299.0488 - mae: 14299.0488 - val_loss: 13449.3906 - val_mae: 13449.3906\n",
            "Epoch 46/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13861.8096 - mae: 13861.8096 - val_loss: 13596.9844 - val_mae: 13596.9844\n",
            "Epoch 47/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15338.1045 - mae: 15338.1045 - val_loss: 12877.6211 - val_mae: 12877.6211\n",
            "Epoch 48/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14991.9971 - mae: 14991.9971 - val_loss: 12791.8711 - val_mae: 12791.8711\n",
            "Epoch 49/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 14213.5918 - mae: 14213.5918 - val_loss: 12876.5244 - val_mae: 12876.5244\n",
            "Epoch 50/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14036.1973 - mae: 14036.1973 - val_loss: 12830.5273 - val_mae: 12830.5273\n",
            "Epoch 51/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14874.8047 - mae: 14874.8047 - val_loss: 13048.7002 - val_mae: 13048.7002\n",
            "Epoch 52/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14725.0000 - mae: 14725.0000 - val_loss: 13405.2129 - val_mae: 13405.2129\n",
            "Epoch 53/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13351.6270 - mae: 13351.6270 - val_loss: 13018.0586 - val_mae: 13018.0586\n",
            "Epoch 54/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14584.9648 - mae: 14584.9648 - val_loss: 13388.4023 - val_mae: 13388.4023\n",
            "Epoch 55/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13947.8350 - mae: 13947.8350 - val_loss: 12813.9072 - val_mae: 12813.9072\n",
            "Epoch 56/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13583.1846 - mae: 13583.1846 - val_loss: 12719.0771 - val_mae: 12719.0771\n",
            "Epoch 57/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13843.0332 - mae: 13843.0332 - val_loss: 12864.8740 - val_mae: 12864.8740\n",
            "Epoch 58/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13542.7930 - mae: 13542.7930 - val_loss: 12785.9287 - val_mae: 12785.9287\n",
            "Epoch 59/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13416.3926 - mae: 13416.3926 - val_loss: 12673.3330 - val_mae: 12673.3330\n",
            "Epoch 60/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 13619.8965 - mae: 13619.8965 - val_loss: 12615.0938 - val_mae: 12615.0938\n",
            "Epoch 61/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13865.6533 - mae: 13865.6533 - val_loss: 13134.3506 - val_mae: 13134.3506\n",
            "Epoch 62/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14559.7119 - mae: 14559.7119 - val_loss: 12679.1787 - val_mae: 12679.1787\n",
            "Epoch 63/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14288.7900 - mae: 14288.7900 - val_loss: 12977.7822 - val_mae: 12977.7822\n",
            "Epoch 64/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14539.6855 - mae: 14539.6855 - val_loss: 12741.3721 - val_mae: 12741.3721\n",
            "Epoch 65/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14278.4688 - mae: 14278.4688 - val_loss: 12687.6406 - val_mae: 12687.6406\n",
            "Epoch 66/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14673.8525 - mae: 14673.8525 - val_loss: 12549.8857 - val_mae: 12549.8857\n",
            "Epoch 67/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14148.2695 - mae: 14148.2695 - val_loss: 12551.2041 - val_mae: 12551.2041\n",
            "Epoch 68/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14505.5791 - mae: 14505.5791 - val_loss: 12920.7783 - val_mae: 12920.7783\n",
            "Epoch 69/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13828.9375 - mae: 13828.9375 - val_loss: 12667.5820 - val_mae: 12667.5820\n",
            "Epoch 70/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13756.1172 - mae: 13756.1172 - val_loss: 13962.2070 - val_mae: 13962.2070\n",
            "Epoch 71/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 14337.3623 - mae: 14337.3623 - val_loss: 13183.0137 - val_mae: 13183.0137\n",
            "Epoch 72/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14767.7168 - mae: 14767.7168 - val_loss: 12523.4414 - val_mae: 12523.4414\n",
            "Epoch 73/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13269.1172 - mae: 13269.1172 - val_loss: 12908.8730 - val_mae: 12908.8730\n",
            "Epoch 74/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 14489.6582 - mae: 14489.6582 - val_loss: 12473.6426 - val_mae: 12473.6426\n",
            "Epoch 75/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13294.5146 - mae: 13294.5146 - val_loss: 12445.1768 - val_mae: 12445.1768\n",
            "Epoch 76/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 14529.5674 - mae: 14529.5674 - val_loss: 13622.6797 - val_mae: 13622.6797\n",
            "Epoch 77/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 13663.7266 - mae: 13663.7266 - val_loss: 12703.4668 - val_mae: 12703.4668\n",
            "Epoch 78/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14413.5703 - mae: 14413.5703 - val_loss: 12861.9883 - val_mae: 12861.9883\n",
            "Epoch 79/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 13844.1035 - mae: 13844.1035 - val_loss: 12831.1338 - val_mae: 12831.1338\n",
            "Epoch 80/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 13921.0654 - mae: 13921.0654 - val_loss: 12989.4551 - val_mae: 12989.4551\n",
            "Epoch 81/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13764.8076 - mae: 13764.8076 - val_loss: 12491.5127 - val_mae: 12491.5127\n",
            "Epoch 82/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13574.0371 - mae: 13574.0371 - val_loss: 12560.0400 - val_mae: 12560.0400\n",
            "Epoch 83/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14417.5303 - mae: 14417.5303 - val_loss: 12576.8135 - val_mae: 12576.8135\n",
            "Epoch 84/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 14177.9834 - mae: 14177.9834 - val_loss: 12490.3975 - val_mae: 12490.3975\n",
            "Epoch 85/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 13971.0400 - mae: 13971.0400 - val_loss: 12436.0234 - val_mae: 12436.0234\n",
            "Epoch 86/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13902.2920 - mae: 13902.2920 - val_loss: 12378.2432 - val_mae: 12378.2432\n",
            "Epoch 87/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13873.4902 - mae: 13873.4902 - val_loss: 12381.1406 - val_mae: 12381.1406\n",
            "Epoch 88/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14856.0566 - mae: 14856.0566 - val_loss: 12619.2373 - val_mae: 12619.2373\n",
            "Epoch 89/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14820.0820 - mae: 14820.0820 - val_loss: 12581.7471 - val_mae: 12581.7471\n",
            "Epoch 90/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14419.2178 - mae: 14419.2178 - val_loss: 12840.1133 - val_mae: 12840.1133\n",
            "Epoch 91/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14103.2002 - mae: 14103.2002 - val_loss: 12554.8057 - val_mae: 12554.8057\n",
            "Epoch 92/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 14378.5752 - mae: 14378.5752 - val_loss: 13266.7695 - val_mae: 13266.7695\n",
            "Epoch 93/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 13873.5713 - mae: 13873.5713 - val_loss: 12793.4688 - val_mae: 12793.4688\n",
            "Epoch 94/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13896.4160 - mae: 13896.4160 - val_loss: 12796.5938 - val_mae: 12796.5938\n",
            "Epoch 95/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13942.0215 - mae: 13942.0215 - val_loss: 12935.2090 - val_mae: 12935.2090\n",
            "Epoch 96/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14696.2510 - mae: 14696.2510 - val_loss: 13509.4326 - val_mae: 13509.4326\n",
            "Epoch 97/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14234.8350 - mae: 14234.8350 - val_loss: 12780.2461 - val_mae: 12780.2461\n",
            "Epoch 98/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14167.1533 - mae: 14167.1533 - val_loss: 12883.1592 - val_mae: 12883.1592\n",
            "Epoch 99/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15089.4023 - mae: 15089.4023 - val_loss: 12650.8027 - val_mae: 12650.8027\n",
            "Epoch 100/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14973.6660 - mae: 14973.6660 - val_loss: 12799.6494 - val_mae: 12799.6494\n",
            "Epoch 101/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13556.4697 - mae: 13556.4697 - val_loss: 12711.1064 - val_mae: 12711.1064\n",
            "Epoch 102/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14494.2119 - mae: 14494.2119 - val_loss: 12682.8613 - val_mae: 12682.8613\n",
            "Epoch 103/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 14190.4502 - mae: 14190.4502 - val_loss: 12736.6475 - val_mae: 12736.6475\n",
            "Epoch 104/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 14896.2842 - mae: 14896.2842 - val_loss: 12808.1279 - val_mae: 12808.1279\n",
            "Epoch 105/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14016.6309 - mae: 14016.6309 - val_loss: 13168.5732 - val_mae: 13168.5732\n",
            "Epoch 106/2000\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 13265.3643 - mae: 13265.3643 - val_loss: 12708.3984 - val_mae: 12708.3984\n",
            "\n",
            "테스트 데이터에 대한 최종 Mean Absolute Error (MAE): $10,680.41\n",
            "-> 모델의 연봉 예측치가 실제값과 평균적으로 $10,680.41 정도 차이남을 의미합니다.\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\n",
            "--- 실제 연봉과 예측 연봉 비교 (상위 5개) ---\n",
            "   Actual Salary  Predicted Salary    Difference\n",
            "0          74266      68167.703125   6098.296875\n",
            "1          59529      45176.625000  14352.375000\n",
            "2          91617      85173.546875   6443.453125\n",
            "3         146692     141366.406250   5325.593750\n",
            "4          71731      91247.007812 -19516.007812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "df = pd.read_csv('/content/ai_job_dataset.csv')\n",
        "\n",
        "korea_df = df[df['company_location'] == 'South Korea'].copy()\n",
        "\n",
        "features = ['required_skills', 'experience_level', 'years_experience']\n",
        "target = 'salary_usd'\n",
        "\n",
        "X = korea_df[features]\n",
        "y = korea_df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "print(f\"훈련 데이터 수: {len(X_train)}개, 테스트 데이터 수: {len(X_test)}개\")\n",
        "\n",
        "text_transformer = CountVectorizer(tokenizer=lambda x: [i.strip() for i in x.split(',')], max_features=300, binary=True)\n",
        "\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "numeric_transformer = MinMaxScaler()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', text_transformer, 'required_skills'),\n",
        "        ('cat', categorical_transformer, ['experience_level']),\n",
        "        ('num', numeric_transformer, ['years_experience'])    ],\n",
        "    remainder='passthrough' )\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(256, activation='linear'),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='linear'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='linear'),\n",
        "    Dropout(0.1),\n",
        "    Dense(32, activation='linear'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mae'])\n",
        "\n",
        "\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('regressor', model)])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "history = pipeline.fit(X_train, y_train,\n",
        "                       regressor__epochs=2000,\n",
        "                       regressor__batch_size=16,\n",
        "                       regressor__validation_split=0.1,\n",
        "                       regressor__callbacks=[early_stopping],\n",
        "                       regressor__verbose=1)\n",
        "\n",
        "# Preprocess X_test before evaluating the model\n",
        "X_test_processed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
        "\n",
        "loss, mae = model.evaluate(X_test_processed, y_test, verbose=0)\n",
        "print(f\"\\n테스트 데이터에 대한 최종 Mean Absolute Error (MAE): ${mae:,.2f}\")\n",
        "print(f\"-> 모델의 연봉 예측치가 실제값과 평균적으로 ${mae:,.2f} 정도 차이남을 의미합니다.\")\n",
        "\n",
        "\n",
        "predictions = pipeline.predict(X_test)\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'Actual Salary': y_test.values.flatten(),\n",
        "    'Predicted Salary': predictions.flatten()\n",
        "})\n",
        "results_df['Difference'] = results_df['Actual Salary'] - results_df['Predicted Salary']\n",
        "\n",
        "print(\"\\n--- 실제 연봉과 예측 연봉 비교 (상위 5개) ---\")\n",
        "print(results_df.head())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPr0YSqeTsd0BE6yN8/gs+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
